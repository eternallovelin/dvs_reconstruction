One of the central problems in robotics and augmented reality is localization:
Knowing where the robot or the user is. To be of any practical use,
localization must be fast and accurate, while relying on few or none external
means.
Vision based systems have been attractive for a long time, for they
provide very rich data with reasonable speed while being very cheap. The
drawback with standard cameras is that high framerates are required to overcome
motion blur, resulting in massive amounts of data. Additionally, the amount of
useful information that can be extracted from standard camera image can rapidly
decrease with difficult, and especially changing illumination conditions. This
leads to the situation that, in many cases, a lot of processing power is
required to extract the relevant information or, in a worst case, no relevant
information can be extracted at all.  There is a new type of digital camera
which overcomes these limitations by closely mimicking a biological retina: A so
called Dynamic Vision Sensor (DVS), which only outputs changes in the sensed
image independently for each pixel.  In this report, we investigate how such a
camera can be used for basic visual SLAM.  This work is heavily based on
\cite{kim2014simultaneous} and essentially shares the same general structure.
