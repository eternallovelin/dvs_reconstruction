Most of our experiments are based on the simulation described in section
\ref{sec:simulation} as our code is not performant enough to handle the amount
of events a real dynamic vision sensor generates and because the simulation
makes comparisons with ground truth much easier.
For the evaluation of the algorithm we simply simulated different camera paths
and ran the reconstruction on the generated data.
Additionally, both the tracking and the reconstruction were evaluated separately
by performing the above experiments and replacing the output of the respective
other part with ground truth data.
In preparation for a usage with real camera data, we performed two additional
experiments to evaluate the feasibility of two ideas that could be used to adapt
the algorithm to the camera data:
A method to generate an image that can be used to initialize the map and a method
to calibrate the camera using a standard camera calibration toolbox.

\subsection{Initialization by Removing Shutter}
\label{sec:shutter_removal}
If the DVS is fully covered by a shutter we can recover a full picture by
simply removing it slowly and integrating all the events generated. This should allow
us to easily create an image for the map initialization using only the DVS.

\subsection{Camera Calibration}
\label{sec:camera_calibration}
As with any sensor, we must calibrate the DVS to get usable data. For normal
cameras, this is usually done by capturing images of a checkerboard pattern from
different viewpoints and subsequently rectifying images so they fit into a
standard pinhole camera model.

Unfortunately, the dynamic vision sensor only detects changes and cannot
directly capture a calibration pattern. We could take pictures as described in
the previous section, but for calibration \cite{mueggler2014event} proposed a different method:
Use a computer screen to quickly show and hide the pattern. With this method,
calibrating the DVS should be almost as convenient as with any standard camera.
The method can also be used to focus the camera optics by displaying a pattern
of lines with varying sizes and observing the resulting camera signal with jAER.
