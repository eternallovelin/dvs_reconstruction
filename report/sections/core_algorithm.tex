Similar to most SLAM approaches, our algorithm consists of two co-dependent
steps. In the first step we try to estimate the current orientation based on
the camera signal, our map of the environment and the previous orientation.
Knowing the position, new data is written to the gradient map and a grayscale
image is reconstructed from these gradients. This iteration is performed every
time an event arrives from the camera. Both the tracking and the rotation work
on the assumption that the output of the other one is correct.

We assume a static scene, so every event (every change in brightness) is caused
by camera rotation or noise.
Further, we only track camera rotation: We assume there is no translation at
all and therefore no parallax displacement either. This greatly simplifies our
algorithm as we only have to keep track of a two dimensional map.
